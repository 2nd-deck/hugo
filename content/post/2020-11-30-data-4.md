---
title: 빅데이터분석 - 4과목 / 빅데이터 결과 해석
date: 2020-11-30
categories: ["No.2 Hold"]
---

제4과목 빅데이터 결과 해석

## 1장 빅데이터 모형평가

### 자료 타당성 검토
1. 데이터 품질 관리
    + 정확성: 분석 대상에 대한 올바른 값 저장
    + 완전성: 결측치 등의 오류가 없음
    + 적시성: 유효한 시간 정보 포함
    + 일관성: 일관된 속성, 규정, 포맷 및 형식 유지
2. 데이터 무결성 검증
- 데이터 무결성: 다수의 사용자가 데이터베이스에 접근하여 조회, 삽입, 삭제, 수정 등의 작업을 수행할 때 데이터가 불일치하지 않는 특성
- 데이터 무결성의 요건
    + 개체 무결성: 기본키는 반드시 값을 가지며 그 값은 유일하고 중복되지 않음
    + 참조 무결성: 외래키 값은 참조하는 테이블의 기본키 값 또는 빈 값 중 하나
    + 속성 무결성: 속성값은 지정된 데이터 형식 만족
    + 키 무결성: 하나의 테이블에 적어도 하나의 키 존재
    + 도메인 무결성: 속성값은 사전에 정의된 도메인 범위의 값
    + 사용자 정의 무결성: 모든 데이터는 업무 규칙 준수
3. 데이터 비식별화
- 데이터에 포함된 개인정보를 삭제하거나 다른 정보로 대체하여 데이터 내에서 특정 개인을 식별하지 못하게 하기 위해 데이터 비식별화를 수행
- 비식별화 방법
    + 가명처리: 식별 가능한 값을 다른 값으로 대체
    + 총계처리: 데이터값을 모두 더해주거나 평균 사용
    + 데이터값 제거: 식별 가능한 값 제거
    + 범주화: 명백한 값을 대체하기 위하여 데이터값을 범주화 함
    + 데이터 마스킹: 식별 가능한 값이 보이지 않도록 처리
4. 데이터 유형별 검증 방법
- 정형 데이터
    + 테이블 목록 확인: 테이블 목록과 원천 데이터 비교
    + 테이블 속성 개수: 테이블별 속성 개수와 원천 데이터 비교
    + 파일의 개수: 전체 파일의 개수와 원천 데이터 비교
    + 파일의 크기: 전체 파일의 크기와 원천 데이터 비교
- 반정형 데이터 
    + 구문 오류: Meta 구조 구문에 대한 오류 검증
    + 데이터 파싱: 파싱  프로그램을 통한 오류 검증
- 비정형 데이터
    + 데이터 크기: 데이터 크기를 샘플링하여 비교
    + 성공 및 실패 검증: 수집 데이터의 성공 및 실패 여부 체크
5. 빅데이터 처리 시스템 요구사항
- 결함 허용 시스템
- 저비용 시스템
- 기존 시스템 연계성
6. 하드웨어
- 마스터 노드: 메모리상에서 처리해야 할 작업이 많아서 고용량 메로리 크기를 요구
- 슬레이브 노드: 실제 파일 블록을 저장하고, 작업을 수행하는 노드이며, 가능한 많은 디스크를 장착하여 실제 블록이 있는 노드에서 작업을 수행하고 노드 사이의 블록 이동은 최소화하도록 설계
7. Spark를 이용한 데이터 분석
- 일반적으로 하둡은 빠른 일괄처리 작업에 유용하지만, 반복적 연산이 많거나 비동기적 처리방식에는 Spark나 Storm을 사용
- 인-메모리 방식의 분산처리시스템으로 기존 맵리듀스 작업에서 발생하는 디스크 I/O에 대한 지연시간을 개선하기 위해 메모리를 사용해서 반복적인 작업이나 스트리밍 데이터를 효율적으로 처리
- RDD(Resilient Distributed Dataset)라는 데이터세트 개념을 도입해서 대용량 데이터 처리를 용이하도록 하였으며, RDD가 메모리 부족으로 적재가 되지 않을 때, HDFS와 연동하여 처리
- 작업 처리는 하둡 자원 관리를 담당하는 YARN(Yet Another Resource Negotiator)이나 자체 개발한 Mesos를 통해서 이루어진다.
8. Storm을 이용한 데이터 분석
- 실시간 데이터 처리 방식의 분산처리 시스템으로 실시간 분석, 온라인 머신러닝, 연속 계산, 분산 RPC, ETL 작업에 유용
- 주요 요소: 님버스(마스터 노드), Supervisors(작업 노드 관리), Zookeeper(클러스터의 조정자 역할), 작업노드(실제 작업 수행)
9. 데이터 검증용 도구의 활용
- GTONE(http://gtone.co.kr) 검증용 자동화 도구는 데이터베이스 관리, 품질 진단, 진단 모니터링 등을 자동 수행하고 그 결과를 보고서 형태로 제공
- 자동화 도구의 기능
    + 진단 대산 데이터베이스 관리
    + 진단 대상 테이블 관리
    + 진단 유형 관리
    + 품질 진단
    + 진단 모니터링
    + 진단 결과 관리
    + 오류 원인 관리
    + 보고서 작성
10. R을 이용한 데이터 검증
- R은 통계 소프트웨어로 다양한 통계 그래픽 기능이 있으며 SPSS, SAS와 같이 통계 계산과 그래픽을 위한 프로그래밍 언어이자 패키지이다.
11. 하둡 시스템에서의 데이터 검증
- 하둡 분산 처리 시스템(HDFS)은 여러 대의 서버로 구성된 클러스터에서 대용량 데이터 처리를 위해서 개발된 분산 파일 시스템이다.
- 데이터 유실이나 하드웨어 고장을 고려하여 복제본을 만들어 여러 노드에 중복 저장. 복제된 블록들은 분산병렬 수행을 지원하고 데이터가 있는 곳에서 계산이 수행
- 블록들은 Master NOde인 Name Node와 Slave Node인 Data Node에 의해서 관리/저장
12. 하둡 에코 시스템을 이용한 데이터 검증, 처리
- Spark
    + 인-메모리 긱반 오픈소스 데이터 분산 처리 엔진
    + 하둡과는 독립적으로 발전한 기술
    + 머신 러닝, 스트리밍 데이터 프로세싱, 그래프 컴퓨팅, SQL 처리
    + HDFS 데이터 연산 수행, Scala로 구현
- Zookeeper
    + 분산 환경에서 노드 간 조정자 역할 수행
    + 노드 사이의 정보 공유, 잠금, 이벤트 등의 기능 수행
    + 각각의 클라이언트들이 동시에 작업 분산 수행, 부하 분산
    + 각 서버에서의 처리 결과를 다른 서버들과 동기화 수행
    + 장애 상황 판단, 복구 기능 수행
    + 하둡 클러스터 구성 컴퓨터와 시스템에서 처리하는 프로그램 관리
    + 클러스터 스토리지 확장 시 컴퓨터 추가, 설정 파일 업데이트 지원
- Yarn
    + 분산 처리, 애플리케이션 작성 지원
- Pig
    + 야후에서 개발한 서비스, 저용 스크립트 이용, 하둡 처리 내용 지시
    + 하이레벨 데이터 플로우 인터페이스 서비스, Pig Latin 언어 사용
    + 프로그래밍에 익숙하지 않은 사용자들을 위한 서비스
    + 데이터 변환 순서 수행, 목적에 맞는 함수 구현 기능
- Hbase
    + HDFS 지원 컬럼기반 DB, 무정지 데이터 저장/백업 지원
    + 대량의 데이터를 처리하는 NoSQL DB 서비스
    + 하둡에서 사용하는 키-값 저장 방식의 데이터 기반 시스템
- Hive
    + 페이스북에서 개발, 대규모 데이터세트 분석, 1차 데이터 가공용 사용
    + Hive QL 명령어 사용, 질의 수행, 데이터 분석 서비스 제공
    + 기본 RDBMS SQL에 익숙한 사용자들에게 거의 동일한 명령어 제공
    + Pig와 비슷하지만 관계형 데이터베이스에 가까움
- Mahout
    + 머신러닝 수행, 클러스터 분석/추천, 맵리듀스 연계, 협업 필터링 적용
- Cassandra
    + HBase와 같은 키-값 저장 방식의 데이터베이스 시스템
- Flume, Sqoop
    + Flume: 외부 데이터 소스의 데이터 통합
    + Sqoop: RDBMS, HDFS 사이 데이터 통합 도구
- Oozie
    + 여러 처리 단계에 대한 워크 플로우 정의, 수행, 작업 스케줄 관리
    + Jobs 사이의 의존 관계 표현, 수행 순서 결정
    + 워크 플로우 관리를 위해 MapReduce, Hive, Pig, Sqoop 등 사용
14. 데이터 3법의 주요 개정 내용
- 데이터 이용 활성화를 위한 가명정보 개념 도입
- 관련 법률의 유사중복 규정을 정비하고 추진체계를 일원화하는 등 개인정보보호 협치 쳬계의 효율화
- 데이터 활요에 따른 개인정보 처리자의 책임 강화
- 모호한 '개인정보' 판단 기준의 명확화

### 모형 타당성 검토
1. 모형 평가 지표 수립
- 통계학은 크게 기술통계와 추론통계로 구분
- 기술통계학은 모집단 전체 또는 표본으로부터 얻은 데이터에 대한 숫자요약이나 그래프 요약을 통하여 데이터가 가진 정보를 정리하는 이론과 방법론을 다룬다.
- 추론통계학은 표본으로부터 얻은 정보를 이용하여 모집단의 특성(모수)을 추론하거나 변수들 간의 적절한 함수 관계의 진위 여부를 판단하는 일련의 과정에 대한 이론과 방법론을 다룬다
- 추론통계학에서는 정규성을 갖는다는 모수적 특성을 이용하는 모수적 방법과 모수적 방법을 사용하지 못하는 경우 자료를 크기순으로 배열하여 순위를 매긴 후, 다음 순위의 합을 통해 차이를 서로 비교하는 비모수적 방법을 이용
- 모집단: 분석을 위해 관심이 있는 대상 전체
- 모수: 모집단의 통계적 속성을 나타내는 수치. (평균, 중앙값, 분산, 표준편차등)
- 표본 추출: 모집단의 부분 집합을 추출한 것. 추출된 표본은 모집단과 같은 대표성을 가진다
    + 단순 무작위 추출(Simple Random Sampling): 정해진 규칙 없이 표본 추출
    + 계통 추출(Systematic Sampling): 모집단을 일정한 간격으로 추출
    + 층화 추출(Stratified Random Sampling): 모집단을 여러 계츠응로 나누고, 계층별로 무작위 추출, 계층은 내부적으로 동질적이고 외부적으로 이질적이어야 함
    + 군집 추출(Cluster Random Sampling): 모집단을 여러 군집으로 나누고, 일부 군집의 전체를 추출
- 탐색적 분석이란 데이터 집합이 실제로 어떤 정보를 포함하고 있는지를 파악하는데 중점을 두고, 통계 기법, 시각화 등을 통해 데이터 집합의 주요 특징을 얻어내는 방법
- 탐색적 데이터 분석을 위한 통계분석 과정
    + 데이터 분석의 목적 설정
    + 분석목적에 맞는 모집단 정의
    + 모집단에 대한 표본 추출을 통해 대표성을 나타내는 데이터를 합리적으로 수집
    + 추출된 표본들을 분석목적에 맞게 층정
    + 데이터 수집 및 저장
    + 통계 기법을 활용하여 데이터 집합의 주요 특징 파악
- 기술 통계학에서 사용하는 주요 수치
    + 평균
    + 중앙값
    + 최빈치
    + 분산
    + 표준편차
    + 범위
- 이외에도 변동계수 CV = 편차/평균, 왜도(분포의 기울어진 정도), 첨도(평균값 주위에 집중적으로 몰려 있는 정도)
- 상관계수: 두 모집단 사이의 선형관계를 나타내는 척도
- 표본상관계수: =피어슨 상관계수. r = 0이면 두 변수 사이에는 상관관계가 없다.
2. 머신러닝을 이용한 예측 기법
- 데이터의 학습 유형에 따라 지도학습, 비지도 학습, 강화 학습으로 분류
- 분류 목적의 머신러닝 알고리즘에서 분석 모형의 성능을 평가하기 위해 혼돈행렬을 사용
    + 예측 범주가 참: 실제 범주가 참이면 TP(True Positive), 거짓이면 FP(False Positive)
    + 예측 범주가 거짓: 실제 범주가 참이면 FN(False Negative), 거짓이면 TN(True Negative)
- 머신러닝의 성능 평가지표
    + 오차비율(Error Rate): (FP+FN)/(TP+FP+FN+TN)
    + 정확도(Accuracy): (TP+TN)/(TP+FP+FN+TN)
    + 민감도(Sensitivity): TP/(TP+FN) 실제가 참인 범주중 참을 올바르게 예측한 비율
    + 특이도(Specificity): TN/(TN+FP) 실제가 부정인 범주중에 부정을 올바르게 예측한 비율
    + 정밀도(Precision): TP/(TP+FP) 긍정으로 예측한 비율중에 실제 긍정인 비율
    + 거짓 긍정률(FP rate): FP/(TN+FP) 실제 부정인 예측중에 잘못 예측한 비율
- F-Measure : 2TP / (2TP+FN+FP) 정밀도와 민감도를 하나로 합한 성능평가 지표. 0~1사이의 범위. 정밀도와 민감도 양쪽 다 클 때 F-Measure도 큰 값을 보임
3. ROC(Receiver Operating Characteristic) 곡선
- ROC는 머신러닝의 성능평가 지표들 중, 거짓긍정률(FP rate)과 참긍정률(TP rate, 민감도)을 이용하여 표현한 곡선이다.
- 혼동행렬 값들 중에서 FP 비율과 TP 비율사이의 관계를 그래프로 표현함으로써 목표변수 범주 값 분류 시 긍정 범주와 부정 범주를 판단하는 기준치의 변화에 따른 참 긍정과 거짓 긍정 비율이 어떻게 변화하는지 알 수 있다.
- 분석 모형에 대한 결과, TP rate 값이 클수록, FP rate 값이 작을수록 성능이 우수하다.
4. k-교차 검증
- k-교차 검증은 데이터 세트를 분할하여 일부 데이터세트로 모델을 훈련하고, 나머지 데이터세트에서 예측력을 평가
- k개의 데이터를 차례로 검증 데이터로 사용하고 나머지를 훈련 데이터세트로 반복적으로 사용하면서 성능을 측정
5. 데이터 마이닝
- 데이터마이닝이란 대용량 데이터로부터 데이터 내에 존재하는 패턴, 관계 혹은 규칙 등을 탐색하고 통계적인 기법들을 활용하여 모델화하며 이를 통해 데이터 분석 및 더 나아가 유용한 정보, 지식 등을 추출하는 과정을 의미한다.
6. 분류를 위한 데이터 분석 모형
- 통계 
    + 다변량 통계이론 적용
    + 로지스틱 회귀분석
- 의사결정나무
    + 트리형태의 분지 방법
    + 의사결정규칙에 따라 관심대상을 몇 개의 소집단으로 분류
    + CART 알고리즘: 각 독립변수를 이분화하는 과정을 반복하여 이진트리 형태를 형성
- 최적화
    + 수리 최적화 기법을 통한 분류 모델
    + SVM(Support Vector Machine) 알고리즘: 최대마진분류기라고도 함. 데이터를 분리하는 초평면 중에서 데이터들과 거리가 가장 먼 초평면을 선택하여 분리, 확정적 모델 기반의 이전 선형 분류
- 기계학습
    + 인공신경망 모델 적용
    + 분류와 예층을 위해 사용
7. 군집화 모형중 k-평균 군집화
- 주어진 군집 수 k에 대하여 군집 내 거리 제곱합 최소화
- 개체점들 간의 거리를 이용하여 전체 데이터세트를 상대적으로 유사한 k개의 군집으로 나눔
- 계산량이 적은 편으로 실행속도가 빠름
- 노이즈 이상치에 민감, 무작위로 군집 중심점을 할당하는 문제로 전역적 최적해가 아닌, 지역적 최적해를 찾기도 함
- 연속형 변수를 이용한 거리 측도에만 사용 가능
- k-평균 클러스터링 알고리즘 수행 절차
    + 군집의 개수 결정 후, 임의의 중심점 n개 설정
    + 데이터 내 각각의 관측값에 대해 n개 중심점까지의 거리 계산 후 가장 가까운 중심점을 각 관측값들의 중심점으로 정함
    + 새로 정해진 중심점에 따라 각각의 데이터 관측값의 소속 군집 할당 각각의 군집 중심점을 다시 계산
    + 새로운 군집 중심점을 이용하여 2, 3단계 반복
    + 관측값들의 군집 할당이 더 이상 변하지 않으면 알고리즘 종료
8. 대표적인 군집 분석 알고리즘
- 계층적 군집: 군집수를 정하지 않고 단계별 군집결과 산출
    + 병합적 또는 상향식 군집: Agglomerative(Bottom-up) Clustering. 모든 데이터 객체를 별개의 그룹으로 구성. 단하나의 그룹화가 될 때까지 각 그룹을 단계적으로 합쳐감
    + 분할식 또는 하향식 군집: Divisive(Top-down) Clustering. 모든 데이터 객체를 하나의 그룹으로 구성. 각 데이터 점이 하나의 그룹으로 될 때까지 단계적으로 분할
- 비계층적 군집: 군집수에 따라 각 객체 중 하나의 소집단으로 배정
    + k-means
    + k-medois: 모든 형태의 유사성 척도 사용. 좌표 평면상 임의의 점이 아닌 실제 데이터 set 내의 값을 사용하여 클러스터 중심 결정. 노이즈나 이상치 처리에 강건한 특징. PAM(Paritioning Around Method)라고도 함
    + DBSAN(Density-based Spatial Clustering of Application with Noise): 밀도개념을 도입, 일정한 밀도로 연결된 데이터 집합은 동일한 그룹으로 판정. 노이즈나 이상치 처리에 강건한 특징
    + 자기 조직화지도(Self-Organizing Map): 자율학습 목적의 머신러닝, 인공신경망의 한 기법
    + Fuzzy: 하나의 객체가 여러 개의 군집에 중복해서 속할 수 있도록 하는 중복 군집화 기법
- 확률기반 군집
    + 가우스 혼합 모형: EM(Expectation Maximization) 또는 MCMC(Markov Chain Monte Carlo)등의 알고리즘 사용. 모수를 추정하는 확률 기반의 군집 분석
9. 연관규칙모형
- 데이터에 숨어 있는 동시에 발생하는 사건 또는 항목들 사이의 규칙을 수치화하기 위해 사용
- 연관성 분석 모형 개념
    + 비지도학습 기법에 해당
    + 방대한 데이터세트에서 객체나 아이템 사이의 연관관계를 찾아냄
    + 빈발 아이템(Frequent Item) 또는 연관 규칙(Associative Rule) 형태로 표현
- 주요 측도
    + 지지도(Support): 전체 데이터세트에서 해당 아이템 집합이 포함된 비율
    + 신뢰도(Confidence): 조건 x를 포함한 아이템 set 중에서 x, y 둘 다 포함된 아이템 set이 발생한 비율
    + 향상도(Lift): 조건 x가 주어지지 않았을 때의 결과 Y가 발생할 확률 대비, 조건 X가 주어졌을 때의 결과 Y의 발생 확률의 증가 비율
    + 향상도가 1을 넘으면 조건과 결과 아이템 집합 사이에 서로 양의 관계, 1보다 작으면 서로 음의 상관관계가 존재
    + 향상도가 1이면 조건과 결과 아이템 집합은 서로 독립적인 관계로 해석
- 연관성 분석 알고리즘
    + 아프리오리: 빈출 아이템 집합을 효과적으로 계산. 검토 대상 집합 Poll을 효과적으로 줄여줌. 데이터가 많아지면 계산량 및 속도 면에서 비효율적
    + 빈출패턴성장: DB를 모든 중요한 정보를 가진 FP-tree라는 구조로 압축
    + 높은 빈도세트와 관련된 조건부 데이터 세트로 분할, 규칙 생성
- 연관성 분석 모형의 활용 분야
    + 방대한 데이터베이스나 인터넷 로그 데이터 등에서 아이템, 객체들 사이의 발생 연관성을 파악하거나 발생순서 간 패턴 파악
    + 쇼핑, 유통 분야에서 구매된 물품 간의 장바구니 분석
- 연관성 분석 활용시 고려사항
    + 항목의 속성 레벨
    + 연관규칙의 비즈니스적 의미 판단
    + 순서 패턴 연관규칙 적용
10. 주요 머신러닝 알고리즘
- 지도학습: 분류가 전형적인 지도학습. 선형회귀, 로지스틱 회귀, SVM, Decision Tree, Random Forest, Neural Network 등이 포함
- 비지도학습: 군집이 해당. 시각화 , 차원축소, 주성분 분석, 연관 분석등이 포함
- 주요 모신러닝 알고리즘
    + 로지스틱 회귀: 설명변수값이 주어졌을 때, 목표변수값이 특정 부류에 속할 확를이 로지스틱 함수 형태를 따름
    + 의사결정나무: 목표변수와 연관성이 높은 순서대로 불순도나 엔트로피 등이 낮아지는 방향으로 나무 형태로 분할하면서 분류 규칙 생성
    + 랜덤포레스트: 주어진 데이터로부터 여러 개의 의사결정나무 생성. 각 의사결정나무의 예측결과를 투표형식으로 집계. 최종적으로 앙상블 모형을 이용한 분류 결과 결정. 
    + 앙상블 모형이란 여러 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법
    + SVM: 서로 다른 분류에 속한 데이터 간의 간격이 최대화가 되는 평면을 찾아 이를 기준으로 분류
    + K-최근접 이웃: 특정 데이터 좌표점과 나머지 좌표점들 간의 거리에 기반을 두어 가장 가까운 k개의 점들의 목적변수값들의 다수결로 분류
    + 나이브베이즈: 베이즈 정리에 근거. 목적변수 발생 조건부 확률을 사전확률과 우도 함수의 곱으로 표현. 관측값들은 서로 독립이라 가정
    + 인공 신경망: 입력, 은닉, 출력 노드 구성. 복잡한 분류나 수치 예측
11. SVM
- 서로 다른 분류에 속한 데이터 간의 간격이 최대가 되는 선을 찾아서 이를 기준으로 데이터를 분류
- 두 범주 간의 데이터를 최대로 나눌 최대 마진 초평면을 찾아서 데이터 분류
- 서포트(지지) 벡터: 경계선과 가장 가까운 각 분류에 속한 점
- 각 분류는 최소 하나 이상의 서포트 벡터를 가짐
- 분류와 수치예측 문제에 활용
- 장점
    + 노이즈 영향을 크게 받지않음. 과적합화가 잘 되지 않음
    + 분류 문제에 대한 성능이 높음
    + 분류 경계가 복잡한 비선형 문제의 경우, 타 기법 대비 성능 우수
- 단점
    + 커널 함수 및 매개변수 등에 대한 반복적인 조합 테스트 필요
    + 입력 데이터 양, 변수가 많으면 오랜 시간의 훈련 소요
    + 다른 기법과 비교하여 난해한 배경 이론 및 알고리즘 구현
    + 결과에 대한 해석이나 이유 설명이 어려움
12. 머신러닝 기법 선정 가이드 라인
- 배경 이론이나 확률적 접근이 가능한 경우 - 로지스틱 회귀, 나이브 베이즈 기법
- 비정형 데이터를 변환하여 분류 예측 등을 수행하른 경우, 분류 결과느 ㄴ중요하나 분류 확률값 자체는 상대적을 중요하지 않을 경우 - 나이브 베이즈 모형
- 분석 모델링에 엄격한 기준이 존재하거나, 모델 결과 계수값등에 대한 유의성 검정이 요구될 경우 - 로지스틱 회귀분석
- 어떤 변수가 목표변수 분류에 중요한지 빠르게 탐색하고자 할 때 -  의사결정트리
- 목표변수 분류 규칙을 도출하거나 지식화 해야 할 경우 -  의상결정나무
- 결과 분류의 이유나 규칙 등에 대한 설명보다는 예측의 정확도가 중요한 경우 - 인공신경망, SVM
- 불완전한 데이터, 노이즈등의 존재, 비정규성 등이 존재하는 데이터일 경우 - 인공신경망, SVM
- 특별한 통계모델의 가정 없이, 또는 모수적인 접근이 아닌 데이터의유사성 등에 근거한 데이터 분류를 원하는 경우 - k-최근접 이웃 기법
- 기존 수행한 분류 모델링의 성능이나 정확도를 개선해야 하거나, 일반화 능력을 높이고자 할떄 - 랜덤 포레스트 등의 앙상블 모형
13. 모형진단 및 시각화
- 데이터 분석 모형의 오류
    + 일반화 오류: 분석 모형을 만들 때 주어진 데이터 집합의 특성을 지나치게 반영. 과적합
    + 훈련 오류: 주어진 데이터 집합에 부차적 특성과 잡음을 고려하여 데이터의 특성을 덜 반영하도록 분헉모형을 만들어 생김. 미적합
- 데이터 분석 모형 검증
    + 홀드아웃 교차 검증: 데이터 집합을 서로 겹치지 않는 훈련집합과 시험집합으로 무작이 구분
    + 다중 교차 검증: 무작위로 동일 크기를 갖는 k개의 부분 집합으로 나누고, 그중 하나를 시험집합 나머지를 훈련집합으로 교차하여 분석 모형
- 데이터 시각화의 기능
    + 설명: 설명적 시각화를 통해 데이터로부터 유의미하거나 흥미로운 이야기와 분석을 명확하게 보여준다
    + 탐색: 탐색적 시각화란 데이터에 숨겨져 있는 관계와 패턴을 찾깅 위한 분석 기능
    + 표현: 표현 기능을 통해 데이터를 활용한 개인 작품이나 예술적 표현으로 이야기 전달, 공감을 불러일으킨다
14. 텍스트 마이닝
- 형태소 분석: 주어진 단어 또는 어절을 구성하는 각 형태소를 분리한 후 태깅 과정 수행
- 키워드 추출: 문서의 중심이 되는 주제어 추출. 불용어 및 가용어 구분. 발생 빈도가 높은 단어들을 키워드로 선정
- 말뭉치 작성
    + 일정 규모 이상의 크기를 갖추고 내용상으로 다양성과 균형성이 확보된 자료의 집합체
15. 과대적합 방지
- 빅데이터 분석 모형의 성능평가를 통해 상대적으로 우수한 모형을 구분하여 사용









## 2장 결과활용 및 시각화