---
title: 빅데이터분석 - 1과목 / 빅데이터 분석 기획
date: 2020-11-15
categories: ["No.2 Hold"]
---

제1과목 빅데이터 분석 기획

## 1장 빅데이터 분석 과제 정의

### 빅데이터 개요
1. 빅데이터 정의
- 대량의 데이터(정형, 비정형등)로부터 가치를 추출하고 결과를 분석하는 기술
2. 빅데이터의 특징 (3V 또는 5V)
- Volume(규모의 증가)
- Variety(다양성)
- Velocity(처리속도)
- Value(가치)
- Veracity(정확성)
- Complexity(복잡성)
3. 데이터의 크기 단위
- Byte, KB, MB, GB, TB, PB, EB, ZB, YB, BB, GeB
4. 공공데이터포털 사이트(http://data.go.kr)
- 데이터3법에 의거 업무의 안정성 제고, 대민 신뢰도 향상 및 투명한 국가 운영을 목표
5. 데이터의 유형
- 정형 데이터: 기반 시스템 관리계, 정보계, 분석계등 업무 시스템 데이터(RDB)
- 반정형 데이터: 정형 구조의 데이터 모델을 준수하지 않는 정형 데이터의 한 형테(Web Log, 보안 및 특정 센서 데이터)
- 비정형 데이터: 형태나 구조가 정형화되지 않은 다양한 형식의 모든 데이터(이미지, 동영상, 센서데이터, SNS데이터등)
6. 빅데이터 요소 기술
- 수집: 데이터 원천으로부터 검색하여 수집, ETL(Extract/Transform/Load) 작업 수행, 크롤링/로그 수집/Open API등
- 저장: 병렬 DBMS, Hadoop, NoSQL등
- 공유: 시스템 간의 데이터 공유
- 처리: 대용량 데이터의 저장, 수집, 관리, 유통, 분석 과정 처리, 분산병렬 처리등
- 분석: 통계분석, 데이터마이닝, 텍스트 마이닝, 최적화 분석등
- 시각화: 차트와 관계 등을 시각화하여 데이터 탐색 및 결과해석 등에 활용
7. 데이터 분석 기획 능력
- NCS(국가직무능력표준, National Competency Stadards)에서는 빅데이터 분석 기획 능력 단위를 네가지 요소로 구분
    + 도메인 이슈 도출하기: 분석과제 AS/IS 및 개선방향 작성, 요건 정의서 수립
    + 분석목표 수립하기: 빅데이터 분석을 통해 얻고자 하는 목표를 정의한 분석목표정의서 수립
    + 프로젝트 계획하기: 프로젝트 계획 설계, 예산/소요기간/현재 환경등을 고려하여 WBS(Work breakdown Structure) 설계
    + 보유데이터 자산 확인하기: 사전 데이터 점검, 데이터 품질/분량/수집 경로 및 데이터 유형 점검
- NCS에서 정의한 빅데이터 분석 직무
    + 대용량의 데이터 집합으로부터 유용한 정보를 찾고 결과를 예측하기 위해 목적에 따라 분석기술과 방법론을 기반으로 정형/비정형 대용량 데이터를 구축, 탐색, 분석하고 시각화를 수행하는 업무
8. 빅데이터와 데이터베이스
- 데이터베이스: 여러 사람에 의해 공유되어 사용될 목적으로 통합하여 관리되는 데이터의 집합
- 데이터베이스 관리 시스템(DBMS): 다수의 컴퓨터 사용자드링 컴퓨터에 수록된 많은 자료들을 쉽고 빠르게 조회, 추가, 수정, 삭제할 수 있도록 해주는 소프트웨어
- 데이터: 수, 영상, 단어 등의 형태로 된 의미 단위이며, 보통 연구나 조사 등의 바탕이 되는 재료
- 정보(information): 특정 목적을 위하여 광 또는 전자적 방식으로 처리되어 부호, 문자, 음성, 음향 및 영상 등을 표현하는 모든 종류의 자료 또는 지식을 의미
- 정보의 주요 특징: 정확성, 적시성, 충분성, 관련성등
- 특정 영역에서 경험을 통해 정보를 통합한 형태를 지식(Knowledge)이라고 한다.

### 분석 목적 설정
1. 빅데이터 분석 및 활용
- 분석의 목적은 기업 및 기관마다 서로 다르다.
- 최근에는 기업에서 제품 차별화, 원가 절감, 소비자 행동 분석 등과 함께 고객관계관리, 고객경험의 변화, 내부 프로세스 및 효율성 개선, 신규 가치 창출 등의 목적으로 빅데이터 분석의 목적을 성정
2. 빅데이터 분석목표정의서
- 작성 방법
    + 분석목적 설정 후, 목적을 이루기 위한 세부 목표 수립
    + 얻고자 하는 목표를 명확히 하기 위해 분석목표정의서를 수립
    + 분석목표 정의서에는 분석별로 필요한 소스 데이터, 분석방법, 데이터 입수 난이도, 분석 난이도, 분석 수행 주기, 분석 결과에 대한 검증이 가능한 성과평가 기준을 설계
    + 도메인 이슈 도출을 통한 개선 방향을 근거로 분석목표를 수정
### 데이터 확보 및 분석 방안 설정
1. 데이터 확보 계획 수립
- 목표 정의 -> 요구사항 도출 -> 예산안 수립 -> 계획 수립
    + 목표 정의: 성과목표 정의, 성과지표 설정
    + 요구사항 도출: 데이터 및 기술 지원등과 관련된 요구사항 도출
    + 예산안 수립: 과제 진행을 위한 자원 및 예산 수립
    + 계획 수립: 인력 투입 방안, 일정 관리, 위험 및 품질 관리
- WBS(Work Breakdown Structure) 작성 단계
    + 데이터 분석과제 정의
    + 데이터 준비 및 탐색
    + 데이터 분석 모델링 및 검증
    + 산출물 정리
2. 데이터 분석 방안 설정
- 빅데이터 분석 절차 6단계
    + 문제인식, 관련 연구조사, 모형화, 자료 수집, 자료 분석, 분석결과 제시
    + 이를 요약하여 4단계: 사용자 요구사항 분석, 모델링, 검증 및 테스트, 적용
- 데이터 마이닝
    + 대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 찾아내는 기법
    + 데이터 구조에 따라 구조화/비구조화 데이터베이스로 나뉜다.
    + 데이터 속의 지식 발견이라 한다.
    + 통계학뿐만 아니라 패턴인식, 기계학습, 인공지능 등의 다양한 영역에서 활용
    + 데이터 마이닝 기법: 분류, 군집화, 연관성, 연속성, 예측
    + 대표적 군집화 방법: K-means Clustering(k-평균 군집화)는 주어진 데이터를 k개의 클러스터로 묶는 자율학습(비지도) 알고리즘
    + 분류 및 예측: 의사결정나무는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타냄

## 2. 빅데이터 분석 계획

### 분석 수행 방안 수립
1. 빅데이터 분석 프로세스
- 문제 인식
- 관련 연구조사
- 모형화(변수 설정)
- 자료수집(변수 측정)
- 자료 분석
- 분석 결과 제시

## 3. 빅데이터 이해
1. 데이터 유형
- 정형 데이터
    + 업무 처리, 매개거래, 로그 데이터, 시계열 데이터
    + DB to DB, Sqoop, EAI
    + ETR(Extract, Transform, Load)등을 이용한 수집
- 반정형 데이터
    + 데이터 내부에 데이터 구조에 대한 메타 정보 포함
    + HTML, XML, JSON, RSS, 웹로그, 보안 및 특정 센서 데이터등
- 비정형 데이터
    + 소셜 미디어, 자유형식 데이터
    + 온토, QR코드, GPS, 오디오, 비디오
    + 데이터 수집: Crawler, FTP, HTTP Protocol수집, Parsing
2. 수집 기술
- 정형 데이터: 관계형 데이터와 분산 환경 데이터 간의 전송 데이터, Sqoop등을 이용해 수집
- 로그 데이터: Flume, Scribe, Chukwa, FTP등을 이용해 수집
- 웹 크롤링, SNS데이터: 웹콘텐츠 데이터 수집, 웹마인닝, Crawler/Scrap/Nutch등을 이용해 수집
- 웹마이닝: 데이터 수집 프로그램을 이용하여 웹페이지로부터 데이터를 수집하고 분석하는 방법
3. 데이터 품질 점검 항목
- 데이터 분량: 필요 칼럼별 확인, 칼럼별 데이터 축적 기간 및 분량
- 데이터 완전성: 데이터 내 필요한 대상과 속성을 포함하는가, 누락 또는 결측값의 비율 확인
- 데이터 일관성: 데이터 속성 간 관계, 상위 하위간 관계에서의 값 일치, 유형과 값의 일치
- 데이터 정확성: 데이터 편향과 분산
4. 데이터 저장
- 데이터 저장 시스템
    + 일반적으로 정형 데이터는 RDB(Relational Database), 비정형은 NoSQL이나 분산파일시스템에 저장하거나 어느 정도 구조적인 데이터 형태로 변형하여 RDB나 NoSQL에 저장
    + 데이터웨어하우스(Data Warehouse): 기업의 의사결정 과정을 지원하기 위한 주제 중심적이고 통합적이며, 시잔성을 가지는 비휘발성 자료의 집합
    + 데이터 마트(Data Mart): 전사적으로 구축된 데이터웨어하우스로부터 특정 주제, 부서 중심으로 구축된 소규모 단일 주제의 데이터웨어하우스이다.
    + 분산파일 시스템: HDFS(Hadoop Distribued File System), GFS(Google File System)
    + RDBMS 방식은 주로 정형 데이터를 저장하고 기존에 운영 중이던 Legacy 시스템으로부터 수집,추출한 데이터를 대량으로 저장할 때 사용
    + NoSQL 방식은 RDBMS보다 상대적으로 제한이 적은 데이터 모델을 기반으로 수평적 확장성, 데이터 복제, 간편한 API, 일관성 보장 등의 장점을 가짐
- NoSQL 종류
    + Key-value DB: 키와 해당 값의 쌍으로 저장하는 데이터 모델( Dynamo, Redis)
    + Column(또는 Row)-oriented DB: Column을 여러 노드로 분할하여 저장하여 데이터를 열 기반으로 저장/확장성 보장(Cassandra, HBase, HyperTable)
    + Document DB: 문서 형식의 데이터 저장, 복잡한 형태의 데이터 저장 지원 및 최적화 가능(MongoDB, SimpleDB, CouchDB)

- 데이터 분산 저장 및 처리
    + 분산 컴퓨팅: 네트워크로 연결된 시스템에 여러 장치를 분산하여 처리, 즉 대용량의 빅데이터 분석을 위해 두개 이상의 컴퓨터를 사용하여 분석 작업을 적절하게 분배하고 그 결과를 조함하여 작업 효율성을 높임
    + 병렬 컴퓨팅: 여러 개의 복잡한 연산을 순차적이 아닌 병렬적으로 동시 처리
    + 클라우드 컴퓨팅: 웹 기반의 컴퓨팅 기술, 인터넷 IT자원(소프트웨어, 플랫폼, 인프라)의 소유가 아니라 대여의 개념
5. 개인정보보호 및 비식별 조치
- 개인정보보호 가이드라인
    + 개인정보 비식별화: 수집 시부터 개인 식별 정보에 대한 철저한 비식별화 조치
    + 투명성 확보: 빅데이터 처리 사실, 목적등의 공개를 통한 투명성 확보
    + 개인정보 재식별시 조치: 개인정보 재식별 시, 즉시 파기 및 비식별화 조치
    + 비밀정보 처리: 민감정보 및 통신비밀의 수집,이용, 분석 등 처리 금지
    + 정보의 기술적, 관리적 보호: 수집된 정보의 저장관리 시 기술적, 관리적 보호조치
- 개인정보 비식별화 절차
    + 사전 검토 -> 비식별 조치 -> 비식별 적정성 평가 -> 사후관리

## 4장 빅데이터 수집 및 추출 생성

### 데이터 식별, 분류

1. 효율적인 데이터 추출/수집을 위한 5단계 프로세스
- 데이터 유형 파악
- 수집 기술 검토
- 수집 솔루션 확인
- 하드웨어 구축
- 실행환경 검토
2. 데이터 측정 척도
- 변수를 측정하는 네가지 척도
    + 명목 척도(Nominal Scale): 측정대상이 어느 집단에 속하는지 분류(ex. 성별, 고객분류)
    + 서열 척도(Ordinal): 측정 대상이 서열관계를 갖는 척도(ex. 고객등급, 순위, 직급, 영화 평점)
    + 등간 척도(Interval): 측정 대상이 갖고 있는 속성의 양 측정, 결과는 숫자로 표현(ex. 온도, 지능지수)
    + 비율 척도(Ratio): 절대적인 영점 존재, 두 측정값의 비율이 의미가 있음(ex. 몸무게, 질량, 개수, 나이)

### 적합한 품질의 데이터로 변환
1. 데이터 변환의 이해
- 목적에 따라 데이터를 변환시켜 주는 것이 데이터 처리, 분석에 효율적
- 일반적인 측정 데이터 변환 방법은 표준화, 정규분포화, 범주화, 개수 축소, 차원 축소, 시그널 데이터 압축 등이 있다.
2. 데이터 변환
- 총계(Aggregation): 두 개 이상의 샘플을 하나의 샘플로 합산하는 방법, y=f(x) 함수를 이용해 변숫값을 일괄 적용하여 새로운 변수를 생성하는 기법
- 평활(Smoothing) 범주화: 데이터 집합에 존재하는 잡음으로 인해 거칠게 분포된 데이터를 매끄럽게 만드는 기법, 구간화(Binning -구간 너비 변경), 군집화(Clustering)등이 사용
3. RDBMS를 이용한 데이터 변환, 저장
- 수집데이터를 변환 저장하기 위해 비정형데이터를 정형 데이터 형태로 저장하는 방식, 수집 데이터를 분산파일시스템으로 저장하는 방식, 주제별/시계열적으로 저장하는 방식, Key값 형태로 저장하는 방식 등을 이용
- 정형 데이터 변환 절차
    + 데이터 구조 정의: 수집 데이터의 속성 구조 확인
    + 수행 코드 정의: 데이터 수집 절차에 대한 수행 코드를 정의, 필요 데이터 추출
    + 프로그램 작성 : 생성된 데이터베이스 테이블에 수집 데이터를 저장하는 프로그램 작성
    + DB 저장: 작성된 프로그램을 실행하여 데이터베이스에 저장
4. 데이터 결합
- 보다 가치 있는 데이터를 추출하기 위하여 정형/비정형 데이터를 결합하여 분석하기도 함.
- 데이터 결합 분석은 크게 4단계
    + 데이터 수집 -> 데이터 처리 -> 분석 및 Insight 도출 -> 조직 내 공유 및 문서화
5. 데이터 품질 검증
- 데이터 품질 수준의 주요 점검 항목
    + 데이터 분량, 완정성, 일관성, 정확성
- 데이터 품질 관리 요소
    + 정확성, 완전성, 적시성, 일관성
- 정형 데이터 품질 기준
    + 정확성, 완전성, 일관성, 유일성, 유효성
- 비정형 데이터 품질 기준
    + 기능성, 신뢰성, 사용성, 효율성, 이식성

## 5장 데이터 정제

### 분석용 데이터세트 정제
1. 데이터 정제 절차
- 데이터 오류
    + 결측치(Missing value): 누락된 변숫값-> 샘플 제거, 해당 변수 제거, 결측치 무시, 결측치 추정
    + 잡음(Noise): 변숫값을 참값에서 벗어나게 하는 오류 -> 구간화, 군집화, 회귀모형 변환
    + 이상치(Anomaly or Outlier): 대부분의 다른 측정값들과 현저한 차이를 보이는 샘플 혹은 변숫값 -> 오류로 판단될 경우 삭제, 특이값인 경우 관심을 두고 분석 수행
2. 데이터 정제 기술
- 데이터 정제
    - 데이터 변환
        + 데이터 유형을 변환 하거나 데이터 분석이 용이한 형태로 변환(비정형 -> 정형)
        + ETL을 통한 동일한 형태 변환
    - 데이터 교정 
        + 결측치 변환, 이상치 제거, 노이즈 데이터 교정
        + 비정형 데이터 수집 시 반드시 수행해야 함.
    - 데이터 통합
        + 용이한 데이터 분석을 위한 기존 유사 데이터와의 연계 또는 통합
        + Legacy system 데이터와 통합하는 경우 수행
- 데이터 처리 방식
    + 대화형 처리: 대용량 데이터 이용, 원하는 질의에 대한 답을 수 초 내에 얻음, 서비스 BI 대시보드 형태로 제공(ex. 하이브 쉘, 임팔라, 피그의 대화형 모드로 처리)
    + 배치 처리: 일일, 주간, 월간 보고서 등 주기적인 작업 수행, 답을 얻기까지 일정한 시간 소요(ex. MapReduce, Hive, Pig, Spark)
    + 실시간 처리: 이벤트성 응답이나 데이터 스트림의 준 실시간 처리를 위해 사용, 결제등에 대한 데이터 분석(ex. Storm, KCL, Spark streaming)
- MapReduce를 이용한 데이터 정제, 처리
    + Map: 분산된 데이터를 키와 값의 리스트로 모음.
    + Reduce: 이들 리스트에서 원하는 데이터를 찾아 처리하는 단계
    + Shuffle and Sort: Mapper로부터 받은 중간 데이터를 정렬하여 리듀서로 전달. 동일한 킷값들은 동일한 리듀서로 전달
    + 맵에서 생성한 결과를 로컬에 저장하고 최종 리듀스 결과는 HDFS에 블록 형태로 저장
    + HDFS -> Map -> Local -> Reduce -> HDFS
    + 매퍼와 리듀서는 프로그래머에 의해 작성된 코드로 처리되고, 셔플과 정렬은 맵리듀스 프레임워크에서 자동 수행
    3. 데이터 세분화(Segmentation)
    - 데이터 세분화란 데이터를 유의미한 기준에 따라 나누는 작업
    - 비정형/반정형 데이터의 경우 사전에 데이터 형식 변환을 통한 세분화 작업이 요구
    - 텍스트의 경우 일반적으로 단어들의 빈도를 표한하는 방법을 이용해 텍스트 덩어리를 정형화된 데이터로 변환한 뒤 텍스트 분석을 수행
    - 이미지의 경우 한 픽셀마다 수치로 변환하는 과정을 거쳐 이미지 분석을 수행. CNN(Convolutional Neural Network 합성곱 신경망)을 주로 사용

    ### 데이터 오류 파악 및 수정
    1. 데이터 결측값 처리
    - 결측값 종류
        + 완전 무작위 결측: 다른 변수와 아무 연관이 없음
        + 무작위 결측: 관측된 변수와 연관이 있지만, 비관측된 값들과 연관이 없음
        + 비무작위 결측
    - 결측값 보완 방법
        + 데이터가 완전 무작위 결측이거나 무작위 결측일 경우 결측 데이터를 삭제하거나 대체해서 데이터를 분석
        + 결측 데이터 삭제: 수집 데이터의 수가 분석하기에 충분하다면 삭제 가능
        + 결측 데이터 보완: 보완 방법에 따라 상당한 오차가 발생
        + 평균치 삽입법: 변수의 평균치를 계산하여 누락된 변숫값으로 사용
        + 보삽법: 시계열 자료의 누락된 데이터 보완
        + 평가치 추정법: 작은 오차만을 감수하면서 원래의 값 추정
    2. 데이터 이상값 처리
    - 이상값은 입력 오류, 데이터 처리 오류등의 이유로 특정 범위에서 벗어나 데이터값
    - 이상값 검출 방법
        + Variance: 정규분포에서 97.5% 이상 또는 2.5% 이하에 포함되는 값
        + Likelihood: 베이즈 정리에 의해 데이터세트가 가지는 두가지 샘플에 대한 발생 확률로 판별
        + Nearest-neighbor: 모든 데이터 쌍의 거리를 계산하여 검출
        + Density: 측정값의 LOF(Local Outlier Factor)를 계산하여 값이 가장 큰 데이터를 이상값으로 추정, 즉 밀도 있는 데이터 세트로부터 먼 데이터가 이상값
        + Clustering: 데이터를 여러 클러스터로 구분한 후 작은 크기의 클러스터나 클러스터 사이의 거리를 계산하여 먼 경우 해당 클러스터에 속한 값을 이상치로 판별
    - 이상값 처리
        + 하한값과 상한값을 결정후 벗어날 경우 하한값 또는 상한값으로 대체
        + 평균의 표준편차로 대체
        + 평균의 절대 편차로 대체
        + 극 백분위수로 대체

